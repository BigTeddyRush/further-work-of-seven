\documentclass[english,version-2020-11]{uzl-thesis}
\usepackage{placeins}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{caption} % Package for captions
\usepackage{subcaption} % Package for subfigures

\usetikzlibrary{shapes.geometric, arrows}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3.5cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{process} = [rectangle, minimum width=3.5cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{optional} = [diamond, dashed, minimum width=3cm, minimum height=1cm, text centered, draw=black, aspect=2]
\tikzstyle{data} = [rectangle, minimum width=3.5cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
\tikzstyle{dashedarrow} = [thick, dashed,->,>=stealth]
\tikzstyle{line} = [thick] % Normal line without arrow


%%%

\UzLThesisSetup{
  %
  %
  Logo-Dateiname        = {uzl-thesis-logo-uzl.pdf},
  Verfasst              = {am}{Institut für Software Engineering und Programming Languages},
  %
  % The titles:
  %
  Titel auf Deutsch     = {
    Können semantische Ähnlichkeiten von Wörtern die Schlussfolgerungen des gesunden Menschenverstands verbessern?  Eine Fallstudie mit Prover E und SUMO.
  }, 
  Titel auf Englisch    = {
    Can semantic similarities of words enhance common sense reasoning?  A case study with prover E and SUMO.
  },
  % 
  %
  Autor                 = {Julian Britz},
  Betreuerin            = {Prof. Dr. Diedrich Wolter},
  % 
  % Optional: Supporting persons and institutions. The text should be
  % in German, even for an English thesis.
  %
  Mit Unterstützung von = {Moritz Bayerkuhnlein},
  % 
  %
  Bachelorarbeit,
  Studiengang           = {Informatik},
  %
  % Date on which the thesis is turned in German, formatted the
  % traditional German way:
  %
  Datum                 = {06. Juli 2025},
  %
  % The English abstract. You must always provide abstracts in German
  % and in English. 
  %
  Abstract              = {
    Abstract
  },
  Zusammenfassung       = {
    Zusammenfassung 
  },
  % Bibliography style: Choose between
  % 
    Alphabetische Bibliographie,
  % Alternatively:
  % Numerische Bibliographie
}

\UzLStyle{alegrya modern design}

\addbibresource{thesis_julian_britz-bibtex-entries.bib}

\begin{document}

%
% The title page and table of contents will be inserted automatically
% here. 
%

\chapter{Introduction}
\label{chapter-introduction}

Automated theorem proving is a fundamental technique in formal logic and artificial intelligence. 
It enables the validation of logical statements based on a given set of axioms and inference rules. 
This thesis investigates whether incorporating frequently used axioms, referred to as core axioms, into a subset of the Adimen SUMO grammar improves proof success rates. 
The selected subset is determined by combining syntactic and semantic criteria to ensure both structural and conceptual relevance.

\section{Problem Statement}
Given a formal logical grammar, such as Adimen SUMO, and a conjecture \( C \), the objective is to identify a subset of axioms that maximizes the probability of proving \( C \). The process involves:
\begin{itemize}
    \item Selecting axioms based on syntactic and semantic similarity, forming a set denoted as \( \mathcal{A}_{\text{rel}} \).
    \item Testing whether adding frequently used axioms, \( \mathcal{A}_{\text{core}} \), improves proof success.
\end{itemize}

Even though there is no direct mapping between natural and logical language, we assume, that there is an entanglement between them: \\
\begin{equation}
    \mathcal{L}_{\text{NL}} \bowtie \mathcal{L}_{\text{logic}}
\end{equation}

Arguments for this assertion are that logical langugage is human made and therefore inevitably connected to human and consequently natural language.
Also grammers like Adimen SUMO are partially derived by lexical-semantic networks that capture relationships between words of natural language.
Making use of this entanglement the selection process of theorem proovers can be optimized by reducing the number of relevant axioms.
Theorem prover E follows a refutational proof procedure using the superposition calculus \cite{Schulz2019}. The proof search is based on a modified given-clause algorithm, consisting of several key steps:

\begin{enumerate}
    \item Initialization: The input problem is converted into clausal normal form (CNF) and divided into two sets: unprocessed clauses \( U \) and processed clauses \( P \).
    \item Clause Selection: A clause \( g \) is selected from \( U \) based on heuristics, referred to as the given clause, and is then simplified against \( P \).
    \item Simplification: The given clause undergoes forward simplification using processed clauses. This step removes redundant literals, applies rewriting rules, and ensures that terms are arranged in a canonical order.
    \item Inference: The prover applies the superposition calculus, combining \( g \) with clauses in \( P \) to generate new clauses. These new clauses are simplified and added to \( U \) if they are neither trivial nor redundant.
    \item Saturation or Termination: The process stops when:
    \begin{itemize}
        \item The empty clause \( \bot \) is derived, confirming the proof.
        \item The clause set is saturated, meaning no further inferences can be made.
        \item A predefined resource limit, such as time or memory, is reached.
    \end{itemize}
\end{enumerate}

Axioms play a crucial role in simplification. While the prover cannot arbitrarily discard axioms, as they form the foundation of logical reasoning, they are essential for clause rewriting and subsumption.


\section{Hypothesis}

To test whether adding frequently used axioms improves proof success, we define the key elements of the problem:

\begin{itemize}
    \item \( \mathcal{A} \) represents the full set of axioms available in Adimen SUMO.
    \item \( C \) is the conjecture we want to prove.
    \item \( d \) is a function that measures how similar an axiom is to the conjecture, based on both syntax and meaning.
    \item \( \mathcal{A}_{\text{rel}} \) is the set of \( k \) axioms that are most similar to the conjecture according to \( d \):
    \begin{equation}
        \mathcal{A}_{\text{rel}} = \{ A \in \mathcal{A} \mid A \text{ is among the } k \text{ closest axioms to } C \text{ w.r.t. } d \}
    \end{equation}
    \item \( \mathcal{A}_{\text{core}} \) is a collection of axioms that have been used frequently in past proofs.
    \item The enhanced axiom set, \( \mathcal{A}_{\text{enh}} \), is created by combining the most similar axioms with the frequently used ones:
    \begin{equation}
        \mathcal{A}_{\text{enh}} = \mathcal{A}_{\text{rel}} \cup \mathcal{A}_{\text{core}}
    \end{equation}
    \item \( P(T, \mathcal{A}', C) \) is a function that checks if the theorem prover \( T \) is able to prove \( C \) using a given set of axioms \( \mathcal{A}' \). If the proof is successful, the function returns true; otherwise, it returns false.
\end{itemize}

The hypothesis we test is:

\begin{equation}
    \forall C \in \mathcal{C}, \quad \Pr(P(T, \mathcal{A}_{\text{enh}}, C) = \text{True}) > \Pr(P(T, \mathcal{A}_{\text{rel}}, C) = \text{True})
\end{equation}

In other words, the likelihood of proving a conjecture is higher when frequently used axioms are included along with the most similar ones.


\section{Justification and Implications}

Adimen SUMO's grammar is closely connected to natural language structures.
This means that selecting axioms based on semantic embeddings, such as SBERT, helps capture underlying logical dependencies that might not be immediately apparent. Frequently used axioms act as structural anchors within this system, making it easier for the theorem prover to navigate the proof search space and discover relevant inference steps.
By combining the most relevant axioms with those that have been frequently used in past proofs, the theorem prover benefits from both semantic alignment and structural consistency. This increases the overall proof success rate, as the prover has access to axioms that not only appear relevant but have also proven useful in previous reasoning tasks.

This thesis tests this hypothesis through empirical experiments, measuring proof success rates across different axiom selection strategies. The goal is to determine how much incorporating core axioms improves theorem proving performance.

\section{Structure of this Work}

This thesis is structured into several chapters, each addressing different aspects of the research problem and methodology. It begins with an overview of existing research, followed by the theoretical foundations, the proposed approach, experimental evaluations, and concluding discussions.

Chapter \ref{chapter-relatedwork} provides a review of related work. It covers previous research on theorem proving and axiom selection techniques, as well as how semantic similarity has been applied in logical reasoning.

Chapter \ref{chapter-preliminaries} introduces fundamental concepts necessary to understand the core of this research. It discusses topics such as common sense reasoning, word embeddings, and theorem proving. Additionally, it explains the role of formal grammars and axiom selection in automated reasoning.

Chapter \ref{chapter-experiments} focuses on the experimental setup and evaluation. It outlines the benchmark datasets, describes the evaluation metrics, and presents the results of different axiom selection strategies. The impact of incorporating core axioms into theorem proving is analyzed based on empirical findings.

Chapter \ref{chapter-conclusion} summarizes the main contributions of this work and highlights key insights gained from the experiments. It also discusses potential future directions and remaining challenges in axiom selection and automated reasoning.

Finally, Chapter \ref{chapter-futerwork} explores ideas for future research, suggesting possible extensions and refinements to the proposed approach.

Each chapter builds upon the previous ones, providing a structured progression from theoretical background to practical implementation and evaluation.

% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\chapter{Related Work}
\label{chapter-relatedwork}

A significant part of this thesis builds upon the work of Schon et al. \cite{Schon2024}, which explores context-specific axiom selection using large language models. Traditional axiom selection methods primarily rely on syntactic properties, often ignoring the meaning embedded in symbol names. Schon proposes an alternative approach by leveraging large language models to determine relevant axioms based on contextual similarity. This method aligns axiom selection with the goal's context, improving performance in commonsense reasoning tasks. Experiments show that this approach outperforms purely syntactic selection methods, demonstrating the potential of semantic similarity for guiding theorem provers.

The findings of Schon's work play a crucial role in this thesis, as a major aspect of the proposed approach involves using semantic embeddings for axiom selection. However, while Schon focuses on direct context-based selection, this thesis investigates the impact of incorporating frequently used axioms into the selection process. The goal is to determine whether combining core axioms with those chosen based on semantic similarity enhances proof success rates.

Beyond Schon's work, several other studies have contributed to axiom selection and theorem proving. Adimen-SUMO, developed by Alvez et al. \cite{Alvez2014}, is a reengineered version of the SUMO ontology designed to improve compatibility with first-order theorem provers. The restructuring ensures that axioms are formulated in a way that facilitates inference, reducing ambiguities present in the original SUMO. This ontology serves as a key resource in evaluating theorem provers, including those used in this thesis.

Syntactic axiom selection has also been widely explored. Hoder and Voronkov \cite{Hoder2011} introduced SInE, a system that selects axioms based on their syntactic relevance to the conjecture. By iteratively choosing axioms that introduce symbols appearing in the conjecture, SInE reduces the search space, making large knowledge bases more manageable. Similarly, Roederer et al.

Another relevant contribution comes from Alvez et al. \cite{Alvez2017}, who introduced a framework for systematic white-box testing of first-order logic ontologies. Their work focuses on ensuring logical consistency and eliminating redundant or contradictory axioms. Automated testing techniques from this study provide insights into ontology refinement, which is relevant when evaluating the quality of selected axioms in theorem proving.

In summary, Schon's research serves as the primary foundation for this thesis, providing a framework for semantic axiom selection that is further extended through the integration of frequently used axioms. Other related studies on syntactic axiom selection, ontology structuring, and logical consistency contribute valuable insights, supporting the refinement of the proposed approach.

% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\chapter{Preliminaries}
\label{chapter-preliminaries}

This chapter introduces fundamental concepts relevant to this work. It covers common sense reasoning, word embeddings, formal grammars, automated theorem proving, and axiom selection strategies. These topics form the theoretical foundation for the experiments and analyses presented in later chapters.

\section{Common Sense Reasoning}

Common sense reasoning is an essential part of human cognition, allowing individuals to understand everyday situations, infer unstated knowledge, and make reasonable decisions even when information is incomplete. Automated reasoning systems attempt to replicate this ability to improve inference and decision-making.

Common sense reasoning involves recognizing unstated premises in logical arguments, understanding cause-effect relationships, and applying heuristics to solve problems under uncertainty. It plays a key role in artificial intelligence, particularly in natural language understanding, where context is crucial for interpreting meaning, and in automated reasoning, where structured logical frameworks help infer conclusions from available premises. Applications in law, medicine, and finance further highlight the importance of integrating knowledge-based reasoning for interpretability and reliability. Despite advancements in AI, deep learning models struggle with incorporating structured and generalizable knowledge, making common sense reasoning an ongoing challenge.

Interpreting a sentence often involves more than just processing its words—it requires incorporating background knowledge to make sense of implicit information. Take, for example, the Winograd Schema involving John and Billy viewing a stage \cite{Levesque2012}. While the sentence itself is simple, understanding it correctly depends on a series of assumptions that humans naturally make when constructing a mental model of the situation \cite{Bayerkuhnlein2023}.

A stage is something meant to be viewed. In a typical setting, viewers remain stationary, and obstructions can block their line of sight. Humans are physical entities, meaning they cannot see through solid objects. Both John and Billy are human names, so the sentence suggests a scene where one person is obstructing the other’s view. The interpretation of who is blocking whom depends on the additional context provided by the sentence, but the reasoning process remains the same: we piece together knowledge about how the world works to arrive at a coherent understanding.

These intuitive assumptions form the foundation of common sense reasoning. The process aligns with the theory of mental models, which suggests that humans construct internal representations of a scenario to evaluate possible interpretations \cite{JohnsonLaird1989}. In the case of ambiguous pronouns, this internal model helps determine which assignment makes the most logical sense. This ability to build structured mental representations is what allows humans to effortlessly resolve ambiguities that challenge AI systems.

\section{Word Embeddings}

Word embeddings represent words as numerical vectors in a continuous space, capturing semantic relationships based on their contextual usage. Unlike traditional representations such as one-hot encoding, embeddings enable words with similar meanings to be positioned closer together in the vector space, allowing models to generalize across linguistic variations. 

Different techniques have been developed to generate word embeddings, each with unique properties and advantages. Word2Vec \cite{Mikolov2013} is one of the foundational models, introducing two architectures: Continuous Bag of Words (CBOW) and Skip-gram. CBOW predicts a word based on its surrounding context, while Skip-gram does the inverse, predicting context words given a target word. This approach efficiently captures both syntactic and semantic relationships. FastText \cite{Bojanowski2017} extends Word2Vec by incorporating subword information, representing words as bags of character n-grams. This allows it to handle rare words and out-of-vocabulary terms better than standard word-based models. GloVe \cite{Pennington2014}, in contrast, constructs embeddings by factorizing a word co-occurrence matrix, capturing both local and global semantic information.

\begin{figure}[h]
    \centering
    \begin{subcaptionblock}{0.45\textwidth}
        \includegraphics[width=\linewidth]{cbow.png}
        \caption{CBOW Model}
    \end{subcaptionblock}
    \hfill
    \begin{subcaptionblock}{0.45\textwidth}
        \includegraphics[width=\linewidth]{skip_gram.png}
        \caption{Skip-Gram Model}
    \end{subcaptionblock}
    \caption{Comparison of CBOW and Skip-Gram Models}
\end{figure}


The effectiveness of word embeddings depends on several factors, including the size of the context window, the type of training data, and the inclusion of subword information.

Beyond standard word embeddings, contextual embeddings such as BERT and GPT \cite{Devlin2019} further refine representation by considering the full sentence structure, allowing word meanings to dynamically change based on context. This advancement is particularly relevant in cases where words have multiple meanings depending on their use in different sentences.

In this work, word embeddings are leveraged to determine the semantic similarity between axioms and conjectures. By using pre-trained models like Sentence-BERT (SBERT), it becomes possible to enhance axiom selection in theorem proving, ensuring that the selected axioms are not only structurally relevant but also semantically meaningful.

\section{Grammars in Natural and Formal Languages}

Grammars define the structural rules of a language, whether it is a natural language like English or a formal system used in logic. In theorem proving, grammars are essential for structuring logical statements in a way that machines can interpret and manipulate.

Different types of grammars serve various purposes. Context-free grammars (CFGs) define syntactic structures in natural language processing and programming languages. First-order logic (FOL) grammars represent logical expressions using predicates, functions, and quantifiers. Ontology-based grammars, such as SUMO (Suggested Upper Merged Ontology) \cite{Niles2001}, combine linguistic and logical structures to improve knowledge representation.

Grammars are crucial in theorem proving because they enforce logical consistency, allow automated systems to process logical expressions systematically, and provide a structured representation of knowledge in formal systems like SUMO and Adimen-SUMO. Adimen-SUMO is written in Thousands of Problems for Theorem Provers (TPTP) Syntax, a standardized language designed for expressing logical problems in automated reasoning \cite{Alvez2014}. TPTP, originally developed to facilitate the benchmarking and comparison of theorem provers, provides a well-defined structure for first-order logic expressions, ensuring compatibility across different proving systems. By using TPTP, Adimen-SUMO maintains a formal and machine-readable representation of axioms, allowing theorem provers to efficiently process and reason over its logical content.

In TPTP syntax, axioms are typically expressed in First-Order Form (FOF), which represents first-order logic statements in a structured and readable format. The FOF format explicitly defines logical components such as axioms, conjectures, and definitions, ensuring clarity in automated reasoning tasks. An example of an axiom in Adimen-SUMO written in FOF syntax is:


\begin{Pseudocode}[morekeywords = {add, create}, deletekeywords={to}, numbers=left,
    caption = {Axiom example Adimen-SUMO}]
    fof(predefinitionsA7, axiom,
        (![X]: 
            (
                p__d__subclass(X, X)
            )
        )
    ).
\end{Pseudocode}



This axiom states that for all entities \( X \), \( X \) is always a subclass of itself. In natural language, this expresses the reflexive property of the subclass relation, meaning that every class is considered a subclass of itself. This kind of axiom is fundamental in ontology-based reasoning, as it helps maintain consistency within hierarchical structures.

\section{Automated Theorem Proving}

Automated theorem proving aims to determine whether a given conjecture follows logically from a set of axioms. It is a key component of formal logic and artificial intelligence.

Theorem provers rely on different proof strategies. Logical inference derives new statements from existing ones using formal rules. Resolution is a proof technique that derives contradictions, commonly used in first-order logic provers. Refutation works by attempting to show that a conjecture holds by deriving a contradiction from its negation.

Prover E is a widely used theorem prover based on the superposition calculus \cite{Schulz2019}. Another prominent theorem prover is Vampire, a first-order logic prover known for its efficient implementation of resolution and superposition techniques \cite{Riazanov2002}. Vampire applies a range of strategies, including term indexing and subsumption algorithms, to optimize proof search. 


\begin{figure}[ht]
    \begin{tikzpicture}[node distance=2.5cm]
        % Nodes
        \node (start) [startstop] {Automated Theorem Proving};
        \node (ontology) [data, below left=1cm and 0.5cm of start] {Ontology};
        \node (conjecture) [data, below right=1cm and 0.5cm of start] {Conjecture};
        \node (axioms) [optional, below=1cm of ontology] {Delimitation};
        \node (theorem) [process, below=3.5cm of start] {Theorem Prover};
        \node (mode) [optional, below=1cm of theorem] {Mode};
        \node (result) [startstop, below=1cm of mode] {Result};
    
        % Connections
        \draw [line] (start) -- (ontology);
        \draw [line] (start) -- (conjecture);
        \draw [dashedarrow] (ontology) -- (axioms);
        \draw [dashedarrow] (axioms) -- (theorem);
        \draw [arrow] (conjecture) -- (theorem);
        \draw [dashedarrow] (theorem) -- (mode);
        \draw [dashedarrow] (mode) -- (result);
    \end{tikzpicture}
    \caption {Automated Theorem Proving}
    \label{fig:automatedTheoremProving}
\end{figure}


\section{Axiom Selection Strategies}

Theorem provers often struggle with large search spaces, making axiom selection a critical challenge. The process of selecting axioms influences proof efficiency and success rates.

\section{Syntax-Based Selection: SInE}

The SInE selection strategy \cite{Hoder2011} is a trigger-based approach designed to efficiently select axioms in automated theorem proving. Instead of selecting axioms based on fixed rules, it dynamically determines relevance using a trigger relation. This relation ensures that an axiom is only selected if at least one of its symbols appears less frequently than or as frequently as all other symbols in the axiom. This prevents highly common symbols from triggering every axiom they appear in, reducing unnecessary selections.

The selection process starts by identifying symbols that appear in the conjecture to be proven. These symbols are considered directly triggered. If a triggered symbol appears in an axiom and satisfies the trigger relation, the axiom is selected, and its other symbols become triggered in the next selection step. This process continues recursively up to a defined depth \( k \), ensuring that only the most relevant axioms are included.

One limitation of this approach arises when an axiom contains symbols with nearly equal occurrence counts. In such cases, only the least frequent symbol can trigger the axiom, potentially excluding useful axioms. To address this, a benevolence parameter \( b \) is introduced, allowing symbols that occur up to \( b \) times more frequently than the least common symbol to also act as triggers. This adjustment provides more flexibility in axiom selection.

\section{Vector-Based Selection}

Syntax-based selection methods like SInE do not consider the meaning of symbol names, which can lead to the loss of useful information. This is particularly problematic in commonsense reasoning, where symbol names often carry semantic meaning that could improve selection. To address this, vector-based selection methods leverage word embeddings to quantify the semantic similarity between axioms and the goal.

As seen above, word embeddings are a common technique in natural language processing, based on the distributional hypothesis \cite{Miller1991}. This hypothesis states that words appearing in similar contexts tend to have similar meanings. By training neural networks on large text corpora, words are mapped into a continuous vector space, where their geometric relationships capture semantic similarity. Cosine similarity is commonly used to measure the closeness of two word vectors, with values ranging from -1 (completely dissimilar) to 1 (identical).

\begin{definition}
    Let \( u, v \in \mathbb{R}^n \) be non-zero vectors. The cosine similarity of \( u \) and \( v \) is defined as:
    \begin{equation}
        \operatorname{cos\ sim}(u, v) = \frac{u \cdot v}{\|u\| \|v\|}
    \end{equation}
\end{definition}

Schon \cite{Schon2023} introduces a statistical approach for axiom selection that relies on word embeddings to represent axioms as vectors. The idea is to encode axioms into a continuous vector space where semantically similar axioms are mapped closer together. This is done in a preprocessing step, where each axiom in the knowledge base is transformed into a vector representation using a word embedding model.

When selecting axioms for a given goal \( G \), the goal is vectorized in the same way as the axioms in the knowledge base. The selection process then identifies the \( n \) axioms whose vector representations are most similar to that of \( G \). The similarity between vectors is measured using cosine similarity, which quantifies how closely two vectors align regardless of their magnitude.

Formally, given a knowledge base \( KB \), a goal \( G \), and a vectorization function \( f \) that maps axioms to \( \mathbb{R}^n \), the vector-based selection process selects the \( n \) axioms in \( KB \) that have the highest cosine similarity to \( G \). The selected axioms form a subset of \( KB \) where no other axiom outside this subset has a higher similarity to \( G \) than the least similar axiom within the selected set.

This approach ensures that axioms most relevant to the goal, based on their semantic similarity, are prioritized during the selection process.

\section{SeVEn: Sentence-Based Vector Encoding}

Sentence embeddings function similarly to word embeddings, as they are also generated by training neural networks on large text corpora. However, while word embeddings focus on individual words, sentence embeddings capture the meaning of entire sentences or even full documents \cite{Kiros2015SkipThought}. This ability makes them particularly useful for encoding more complex relationships between axioms.

Following this idea, the SeVEn approach builds upon vector-based selection by modifying how axioms are represented. Instead of encoding individual symbol names, each axiom is first translated into a sentence that captures its semantic meaning. This sentence is then encoded into a vector using a sentence embedding model. The overall process remains similar to standard vector-based selection, but the vectorization method is adapted to work at the sentence level. Given a knowledge base \( KB \), where each axiom \( A \) is transformed into a sentence \( S \) using a function \( t \), the sentence embedding function \( f \) then maps \( S \) into a vector representation \( v_S(A) \). The full sentence-based vector representation of the knowledge base is denoted as \( V_S(KB) \), where each axiom is now represented as a sentence embedding.

This adaptation allows SeVEn to leverage the richer semantic information captured in sentence embeddings while maintaining the core structure of vector-based selection.

\section{Combining Vector-Based and Syntactic Selection}.

To address this issue, hybrid selection strategies combine statistical and syntactic approaches. An example of this is Similarity SInE \cite{Furbach2019WordEmbeddings}, which extends the original SInE selection by allowing not just the least frequent symbol to trigger an axiom but also symbols with a high enough cosine similarity to the trigger. This modification ensures that semantically related axioms can still be included in the selection process.

Building on this idea, \cite{Schon2023} introduced a hybrid approach that integrates SeVEn with the syntactic selection method of SInE. The process starts by expanding the goal with axioms selected through SeVEn. Then, SInE is applied to this extended set of conjectures, selecting additional axioms based on its trigger-based mechanism. 

Using formal notation, let \( \text{sine}(KB, \{A_1, ..., A_n\}) \) represent the axioms selected by SInE for a set of conjectures, and let \( \text{seven}(KB, G) \) denote the axioms selected by SeVEn for a goal \( G \). The combined selection strategy is then defined as:

\begin{definition}
    \begin{equation}
        \text{union\_select}(KB, G) = \text{sine}(KB, G \cup \text{seven}(KB, G))
    \end{equation}
\end{definition}

This formulation allows for flexibility in axiom selection, as different selection strategies can be incorporated depending on the specific needs of the reasoning task.

% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\chapter{Experiments}
\label{chapter-experiments}

This chapter presents the experimental setup and evaluation of different axiom selection strategies in the context of automated theorem proving. The experiments focus on white-box testing, reengineering of prior work, comparative analysis of selection methods, and performance assessments across different theorem provers.

\section{Construction of White-Box Tests}

The methodology for constructing white-box tests follows the approach described by Alvez \cite{Alvez2017}. The tests are designed to assess the logical consistency and inference capabilities of theorem provers by introducing controlled variations in axiom selection.

In the context of Adimen SUMO, falsity tests are constructed by taking specific statements or conditions and applying negation. These negated statements form the basis for falsity tests, which are essential for evaluating logical proof systems. A theorem prover such as Prover E processes these tests through a method called refutation. The goal of refutation is to show that the original statements are inconsistent or false. This is done by attempting to derive a contradiction from the negated statements. If a contradiction is found, it confirms that the original, non-negated statements cannot all be true at the same time.

An example of a white-box falsity test in Adimen-SUMO is the following:

\begin{Pseudocode}[morekeywords = {add, create}, deletekeywords={to}, numbers=left,
    caption = {WhiteboxTruthTest example}]
    fof( whiteBoxTruthTest2824, conjecture,
        (?[MORPH]: 
            (
                ~ (
                    p__d__instance(MORPH,c__Morpheme)
                )
            )
        )
    ).
\end{Pseudocode}


The expression states that there exists at least one entity, represented by the variable \( MORPH \), for which the predicate \( p\_\_d\_\_instance(MORPH, c\_\_Morpheme) \) does not hold. In simpler terms, it asserts that at least one entity is not an instance of the class "Morpheme."

By negating the statement that all instances belong to the class "Morpheme," this test challenges the theorem prover to derive a contradiction if the ontology implies that every entity must belong to this class. If a contradiction is found, it confirms that the original classification of "Morpheme" is inconsistent, helping to assess the reasoning capabilities of the prover.

\section{Reengineering of Prior Work}

\section{Reengineering of Axiom Selection Results}

To evaluate different axiom selection techniques, this work reengineers the results presented in \cite{Schon2024}. The primary goal is to validate previous findings while assessing the impact of alternative selection methods. The evaluation compares three selection approaches: syntactic selection using SInE, semantic selection based on vector embeddings, and a hybrid union approach combining both methods.

Each method is tested under the same conditions using a randomly selected subset of 1,000 white-box truth tests from Adimen-SUMO. The theorem prover E is used with a fixed configuration, ensuring consistent input formatting and a time limit of 15 seconds per proof attempt:

\begin{Pseudocode}[morekeywords = {add, create}, deletekeywords={to}, numbers=left,
    caption = {Prover E configuration}]
    eprover --tstp-format --soft-cpu-limit=15
\end{Pseudocode}

For syntactic selection, the SInE strategy is applied with a benevolence parameter of 3 and a recursion depth of 2. In contrast to prior work, the number of selected axioms in the semantic analysis is increased to 1,500, which is three times higher than the previous maximum. This adjustment is made to align the number of selected axioms with the average output of the SInE strategy, ensuring comparability between the syntactic and semantic runs.

The hybrid union approach, which combines vector-based and syntactic selection, is configured as follows:

\begin{itemize}
    \item The sentence embedding model used is the pre-trained all-MiniLM-L6-v2 from SentenceTransformer, featuring 384 dimensions and a model size of 80MB. It has been trained on a large dataset of over 1 billion training pairs and is optimized for diverse NLP tasks \cite{Reimers2019, Sentencetransformers2019}.
    \item The number of closest axioms \( n \) selected for each goal \( G \) is set to 160.
    \item The SInE parameters remain the same as in the syntactic selection: benevolence = 3, recursion depth = 2.
\end{itemize}

Since the union approach applies the SInE strategy to a broader set of initial axioms than the purely syntactic selection, the final number of selected axioms is also higher, averaging around 1,800. This increase reflects the integration of both semantic and syntactic relevance in axiom selection, aiming to enhance the theorem prover’s ability to find proofs efficiently.

The prover results are categorized into:
\begin{itemize}
    \item \textit{Timeout:} Instances where the automated prover exceeded the allocated computational resources without deriving a proof.
    \item \textit{Proof Found:} Instances where the prover successfully established a proof within the given constraints.
    \item \textit{Gave Up:} Instances where the prover terminated prematurely without reaching a conclusive result.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{standard_mode_noAdded_output.pdf}
    \caption{Reengineering of results from \cite{Schon2024}}
    \label{fig:reengineering}
\end{figure}

The results in \ref{fig:reengineering} correspond to the prior investigations. While a purely syntactic and purely semantic approach can only find few proofs in the given time, the combination can achieve better results.
A significant portion of test cases resulted in timeouts or failed proof attempts. But also for the combination the overall success rate remained low.
The findings suggest that relying solely on semantic similarity in combination with a syntactical selection precedure is not sufficient for effective axiom selection. Future experiments should investigate whether incorporating frequently used core axioms can enhance proof success rates. Additionally, exploring the relationship between axiom complexity and proof outcomes may offer deeper insights into theorem proving behavior, ultimately leading to more refined selection strategies.


\section{Analysis of Reengineering}

To get an indication and possibly recognize patterns for which conjectures the prover could find proofs, the results of the previous union run are examined. To understand the factors affecting the success of proof search in theorem proving, several characteristics of the conjectures are examined. Specifically, the influence of the number of variables, the presence of logical operators, and the overall character count in a conjecture are analyzed. These elements contribute to the complexity of the search space and potentially impact the likelihood of finding a proof.

Since each variable expands the number of possible substitutions and inferences the prover must consider, a higher number of variables generally results in a larger search space. It is expected that conjectures with fewer variables are easier to prove, as the prover has to evaluate fewer possible cases.

First-order logic relies on specific symbols as logical operators, each contributing to the structural complexity of a conjecture. The most relevant symbols include:
\begin{itemize}
    \item \texttt{|} (disjunction) – represents logical OR.
    \item \texttt{\$} (conjunction) – represents logical AND.
    \item \texttt{?} (variable indicator) – denotes the existence of a variable.
    \item \texttt{!} (universal quantifier) – states that a property holds for all instances of a variable.
    \item \texttt{\~{}} (negation) – expresses logical NOT.
\end{itemize}
A higher occurrence of these symbols increases the structural complexity of a conjecture. More complex logical expressions require additional inference steps, making it more challenging for the theorem prover to find a proof. Therefore, it is expected that conjectures with fewer logical operators are more likely to be proven within a limited time.

Lastly, the overall character count of a conjecture is analyzed as a general measure of complexity. Longer conjectures tend to introduce more structural elements, increasing the reasoning effort required by the prover. By examining how the length of a conjecture correlates with proof success rates, it is possible to determine whether shorter conjectures are indeed easier to prove.

The following presents the results of these analyses, comparing proof success rates based on these complexity factors. The implications of these findings are then discussed in detail, highlighting how different structural properties influence theorem proving performance.


\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{combined_graphs.pdf}
    \caption{Analysis of Reengineering}
    \label{fig:analysis_reeingieering}
\end{figure}
\FloatBarrier

The results confirm that all examined factors influence the success rate of theorem proving as expected. Conjectures with fewer variables consistently yielded higher proof success rates, supporting the assumption that an increase in the number of variables expands the search space, making proof discovery more challenging.

Similarly, the impact of logical operators aligns with expectations. Conjectures containing a higher number of disjunctions, conjunctions, quantifiers, and negations were significantly harder to prove.

The general character count of a conjecture also showed a correlation with proof success. Conjectures with a lower character count were proven more frequently, reinforcing the idea that shorter conjectures introduce fewer structural elements that could complicate reasoning. While character count alone is not a direct measure of complexity, it serves as a useful indicator of the structural and syntactic complexity of a logical statement.

Next to analyze how the cosin similarity of the 160 chosen axioms differs between those proofs, for which a proof could be found and those not.

Next, the cosine similarity of the 160 selected axioms is analyzed to compare cases where a proof was successfully found to those where no proof could be derived. This evaluation aims to determine whether higher semantic similarity between the selected axioms and the conjecture correlates with a higher proof success rate.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{cosine_similarity_mini_noAdded_summary.pdf}
    \caption{Cosine similarity distribution}
    \label{fig:cosine_similarity}
\end{figure}
\FloatBarrier

Unexpectedly, the average cosine similarity between the conjecture and the 160 nearest axioms is slightly lower in cases where a proof was found compared to those where no proof was derived. This suggests that an excessively high cosine similarity may restrict the search space too much, making the selection of axioms overly specific and limiting the prover's ability to construct a valid proof. This indicates that proving a conjecture may require bridging different concepts rather than simply refining closely related axioms.


\section{Expanding Axiom Selection Beyond Semantic and Syntactic Methods}

As previously observed, the pure combination of syntactic and semantic selection methods does not cover a sufficiently broad range of proofs. The results show that conjectures without a successful proof tend to have a higher cosine similarity to their 160 nearest axioms. This suggests that relying solely on semantic and syntactic similarity is not enough. A too-specific selection of axioms can lead to missing essential axioms, ultimately preventing the prover from finding a proof.

Since theorem provers, as described in \ref{chapter-introduction}, rely on simplification steps using available axioms, it becomes evident that additional axioms enabling these simplifications are required. Without such axioms, the prover lacks the necessary transformation rules to progress toward a proof.

To identify the core axioms, Prover E was executed in auto mode across all 8,010 white-box truth tests. In this mode, the prover automatically determines the optimal SInE parameters and applies a search heuristic best suited for each conjecture. After running all tests, the 25 axioms that appeared most frequently in successful proofs were selected as core axioms.

\begin{Pseudocode}[morekeywords = {add, create}, deletekeywords={to}, numbers=left,
    caption = {Prover E configuration}]
    eprover --auto --tstp-format --soft-cpu-limit=15
\end{Pseudocode}


The choice of these axioms is based on their practical relevance in proof construction. Since they are consistently used in successful proofs, they likely provide essential logical connections and simplifications that improve the theorem prover's efficiency.

An example of a core axiom from Adimen-SUMO that facilitates inference is:


\begin{Pseudocode}[morekeywords = {add, create}, deletekeywords={to}, numbers=left, caption = {Example core axiom}]
    fof( mergeA594, axiom,
        (![REL]: 
            (
                (
                    p__d__instance(REL,c__BinaryRelation)
                )
                =>
                (
                    (
                        p__d__instance(REL,c__IrreflexiveRelation)
                    )
                    <=>
                    (
                        (![INST]: 
                            (
                                ~ (
                                    p__d__holds3(REL,INST,INST)
                                )
                            )
                        )
                    )
                )
            )
        )
    ).
\end{Pseudocode}


This axiom defines the irreflexive property of certain binary relations. It states that if a relation REL is classified as a binary relation, then it is an irreflexive relation if and only if there exists no instance INST for which REL holds between INST and itself. In simpler terms, this means that for a relation to be irreflexive, it must never relate an entity to itself.

Such an axiom is fundamental in reasoning about relational structures, as many logical proofs rely on distinguishing between reflexive and irreflexive relations. Without it, the prover may struggle to validate properties of relations that are essential for defining hierarchical and ordering constraints. Including these core axioms in the selection process ensures that theorem proving is not only driven by semantic and syntactic similarity but also by foundational logical principles necessary for accurate inference.

To increase the likelihood of selecting relevant core axioms without significantly expanding their number, a SInE strategy was applied to the 25 core axioms with a benevolence value of 1 and a recursion depth of 1. The axioms selected through this process were then combined with those obtained from the union-based selection method, ensuring that both frequently used and structurally relevant axioms were included. Applying this on the same 1.000 WhiteboxTruthTests as before, 286 proofs could be found.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{standard_mode_output.pdf}
    \caption{Summary of prover results with core axioms}
    \label{fig:prover_results_with_core_axioms}
\end{figure}
\FloatBarrier

The union run successfully found 123 proofs. After adding the core axioms, the number of proofs increased by 163, resulting in a total of 286 proofs. A closer analysis showed that all 123 proofs from the union run were a subset of the proofs found when core axioms were included. 

\begin{equation}
    P_{\text{union}} \subseteq P_{\text{core}} \cup P_{\text{union}}
\end{equation}
where \( P_{\text{union}} \) represents the set of proofs found in the union run and \( P_{\text{core}} \) represents the set of proofs found after adding core axioms.


This suggests that adding core axioms plays a crucial role in enabling the prover to find proofs that would otherwise remain undiscovered.

\section{Time analysis}
\section{Impact of Search Space Limitation on Proof Time}

To determine whether restricting the search space influences the time required for Prover E to find a proof, the time taken in different configurations is analyzed. The evaluation compares three scenarios: standard mode, union mode, and union mode with added core axioms.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{time_to_find_proof.pdf}
    \caption{Time required for Prover E to find a proof}
    \label{fig:time_different_mode_1}
\end{figure}        
\FloatBarrier

The results show that limiting the search space significantly improves proof discovery speed. In union mode, where axioms are pre-selected using syntactic and semantic strategies, the prover operates more efficiently by focusing on a smaller yet relevant subset of axioms. However, when core axioms are added, the search space expands slightly, leading to a marginal increase in proof time. Despite this, the overall performance remains better than in the standard mode, indicating that reducing unnecessary axioms outweighs the added complexity introduced by core axioms.

To further examine the effect of search space limitation, the time required to find proofs common to different configurations is compared. The following figure presents the average time taken by the first method listed to find the same proofs that were already identified by the second.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{time_to_find_common_proof.pdf}
    \caption{Average time required to find common proofs}
    \label{fig:time_different_mode}
\end{figure}        
\FloatBarrier

The trend remains consistent—limiting the search space leads to faster proof discovery. When axioms are pre-selected, Prover E spends less time exploring irrelevant information, allowing it to reach conclusions more efficiently. These results further support the importance of a well-structured selection process in optimizing theorem proving performance.


\section{Comparison of Large Language Models}

As previously observed, adding frequently used axioms to the set selected by the union process significantly increases the number of proofs that Prover E can find. The selection process relies on word embeddings to determine semantic similarities between axioms and conjectures. In the initial setup, a model embedding into 384 dimensions was used. To assess whether the number of dimensions and the size of the model impact proof discovery, the same procedure is repeated using a different model, all-mpnet-base-v2.

The two models differ in several key aspects, including sequence length, embedding dimensions, and overall model size. The following table provides a comparison:

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \hline
        & \textbf{all-mpnet-base-v2} & \textbf{all-MiniLM-L6-v2} \\
        \hline
        Max Sequence Length & 384 & 256 \\
        Embedding Dimensions & 768 & 384 \\
        Model Size & 420MB & 80MB \\
        Training Data & 1B+ training pairs & 1B+ training pairs \\
        \hline
    \end{tabular}
    \caption{Comparison of embedding models used in axiom selection.}
    \label{tab:model_comparison}
\end{table}

The difference of the two models can be seen by the mean sinus similarity of the sample whiteboxtruth tests to all axioms in the Adimen Sumo Ontology.


\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \hline
        & \textbf{all-mpnet-base-v2} & \textbf{all-MiniLM-L6-v2} \\
        \hline
        Mean cosine similarity & 0.23 & 0.18 \\
        \hline
    \end{tabular}
    \caption{Mean cosine similarity of test-sample to ontology}
    \label{tab:model_comparison}
\end{table}

The all-mpnet-base-v2 model exhibits a significantly higher mean cosine similarity between the sample test conjectures and all axioms in Adimen-SUMO. As previously observed, a high cosine similarity can restrict the search space for the prover, making it necessary to include core axioms to improve proof success. To examine this effect, the prover was run in union mode using the all-mpnet-base-v2 model and again in union mode with the addition of the previously identified core axioms.

The results of theorem proving with different models are presented in the following figure:

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{different_mode_output.pdf}
    \caption{Summary of prover results with different models}
    \label{fig:results_different_models}
\end{figure}        
\FloatBarrier

The increased cosine similarity resulted in fewer proofs being found in union mode. The all-mpnet-base-v2 model produced only 97 proofs in the union run, compared to 123 proofs found using the smaller model. Similarly, the number of proofs in the union run with added core axioms was lower than in the corresponding run with the smaller model. Despite this, adding core axioms still led to an increase of 154 additional proofs, reinforcing their importance in theorem proving.

The results suggest that while the larger model provides more accurate embeddings, its higher semantic precision appears to require a broader or more refined selection of axioms to perform as effectively as the smaller model. This indicates that adjustments in axiom selection strategies may be necessary when using higher-dimensional embeddings to ensure optimal prover performance.


\section{Evaluation of Theorem Prover Configurations}

Up to this point, all runs have been conducted using Prover E's standard mode without any specific configurations. To determine whether adding core axioms remains beneficial when the prover operates under specialized settings, two different modes of Prover E will be examined.

Prover E offers specialized modes that automatically adjust search parameters based on the characteristics of the given problem. One such mode, \textit{--satauto}, dynamically configures various settings, including literal selection strategies, clause evaluation heuristics, and term ordering, to optimize proof search. This mode adapts to the problem structure, aiming to improve efficiency without requiring manual parameter tuning.

\begin{definition}
    \textit{--satauto mode}: Choose literal selection strategy, clause evalua-
    tion heuristic, term ordering and other search
    parameters automagically, based on problem fea-
    tures \cite{schulz2019eprover}.
\end{definition}

Another specialized mode offered by Prover E is \textit{--auto} mode, which builds upon the functionalities of satauto by incorporating heuristic pruning. In addition to automatically selecting search parameters, this mode applies an instance of the SInE algorithm to manage large specifications more efficiently. While this can improve performance by reducing the search space, it also introduces the possibility of incompleteness, meaning that some proofs may not be found even if they exist.

\begin{definition}
    \textit{--auto mode}: As --satauto, but add heuristic specification
    pruning using one of several instantiation of the
    SInE algorithm for large specifications.
    This makes the prover potentially incomplete \cite{schulz2019eprover}.
\end{definition}
The impact of different selection strategies on theorem proving performance is evaluated by comparing Prover E's satauto mode under three conditions: running on the full ontology without pre-selection, using union mode to pre-slice the search space, and applying union mode with the addition of core axioms. The results are summarized in the following figure:

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{satauto_mode_output.pdf}
    \caption{Summary of prover results in Satauto mode}
    \label{fig:prover_results_satauto}
\end{figure}
\FloatBarrier

With 310 proofs found, it is evident that automatic selection strategies for literal selection, clause evaluation, and term ordering significantly impact the prover's performance. However, reducing the search space using a combination of syntactic and semantic selection further improves the results. In union mode with added core axioms, the prover successfully found 509 out of 1,000 proofs, marking the first case where more proofs were found than not, reinforcing the effectiveness of this approach.

The same evaluation is then performed in auto mode, where Prover E additionally applies a SInE strategy for heuristic pruning.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{auto_mode_output.pdf}
    \caption{Summary of prover results in Auto mode}
    \label{fig:prover_results_auto}
\end{figure}
\FloatBarrier

The results are comparable to those in satauto mode. Since the search space was pre-sliced before execution, the additional heuristic slicing in auto mode does not significantly alter performance.

Overall, restricting the search space through a combined syntactic and semantic selection process, supplemented by core axioms, consistently improves the prover’s performance. These results highlight the importance of carefully structuring axiom selection to balance relevance and efficiency in theorem proving.

\section{Different prover}
To evaluate whether the proposed adjustments are effective across different theorem provers, the same process is applied using the Vampire prover. By default, Vampire runs in an optimized mode that automatically selects the best configuration, making it comparable to the auto mode in Prover E. As before, the prover is tested in three scenarios: running on the full ontology without pre-selection, using union mode, and applying union mode with added core axioms.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{vampire_mode_output.pdf}
    \caption{Summary of prover Vampire}
    \label{fig:prover_results_vampire}
\end{figure}
\FloatBarrier

Similar to Prover E in satauto and auto mode, Vampire successfully finds a large number of proofs in its default configuration. However, the results show the same trend as before—both union mode and union mode with core axioms further improve performance. This confirms that the approach is not specific to Prover E but generalizes well across different theorem provers.
 

% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
% !!! Your action is needed here !!!
% !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\chapter{Conclusion}
\label{chapter-conclusion}
This thesis examined the impact of incorporating frequently used axioms, referred to as core axioms, into the axiom selection process of automated theorem provers. The hypothesis was that adding these axioms, which appear frequently in successful proofs, would improve proof success rates by providing necessary inference steps. To evaluate this, a combination of syntactic and semantic axiom selection strategies was explored, and the results were analyzed across different configurations and theorem provers.

The experiments showed that purely semantic and syntactic selection strategies are insufficient for maximizing proof success. While semantic embeddings allow for meaningful axiom selection, the results indicated that an overly strict focus on similarity can restrict the prover’s search space too much, limiting proof discovery. The syntactic approach, although effective in reducing the search space, also showed limitations when used alone. The integration of both methods in a union-based approach improved performance, but adding core axioms led to the most significant increase in proofs found.

The evaluation confirmed that theorem provers benefit from structured axiom selection, especially when core axioms are included. In standard mode, Prover E found 123 proofs using union-based selection, while the addition of core axioms increased this number to 286. Similar trends were observed across different prover settings, including satauto and auto modes, as well as in the Vampire prover. In all cases, restricting the search space through a combination of selection strategies enhanced proof success while maintaining efficiency.

Another key finding was the relationship between search space limitation and proof time. The results showed that selecting relevant axioms reduced the time needed to find proofs. However, adding core axioms slightly increased proof time due to the expanded search space, though the overall efficiency gain from improved proof success outweighed this effect.

An additional comparison was conducted using different embedding models to assess whether model size and embedding dimensionality influence theorem proving performance. The all-mpnet-base-v2 model produced embeddings with higher cosine similarity between conjectures and axioms than the all-MiniLM-L6-v2 model. However, the increased similarity resulted in fewer proofs being found, reinforcing the observation that a balance between similarity and search space flexibility is needed. 

The findings demonstrate that effective axiom selection is critical in automated theorem proving. The combination of syntactic filtering, semantic embeddings, and frequently used core axioms provides a well-structured selection process that enhances proof discovery. These results suggest that theorem proving can be further optimized by refining axiom selection methods, balancing search space reduction with the need for inference-enabling axioms.

\clearpage

\chapter{Future work}
\label{chapter-futerwork}

While the current approach demonstrates the effectiveness of combining syntactic and semantic selection with core axioms, several aspects of the configuration could be further optimized. One key area for improvement is analyzing the impact of different parameter settings, such as the number of closest axioms \( n \) and the specific SInE parameters used. Adjusting these values dynamically based on the complexity of the conjecture or the structure of the selected axioms could lead to further performance gains.

Another promising direction is the dynamic selection of core axioms. The current method identifies frequently used axioms across multiple proofs, but this selection remains static. A more adaptive approach could refine core axioms on a per-conjecture basis, ensuring that the most relevant axioms are always included. This could involve techniques such as iterative refinement or heuristic weighting based on past proof success.

Exploring these areas could lead to a more flexible and efficient selection strategy, further improving theorem proving performance while maintaining a balanced search space.




% Normally, the bibliography comes next at this point. Do *not* (try
% to) include further indices and tables like an index or
% a list of figures or a list of tables or such things. Nobody
% actually uses them and they just use up space. 
%
% You *can* however include a glossary, if this seems appropriate. It
% goes here as an unnumbered chapter. Most thesis will *not* need a
% glossary: a well-written text (re)explains strange words and
% concepts as necessary. However, there are situations where a
% glossary may be helpful.

%%%
% 
% Bibliographies
%
%%%
%
% The uzl-thesis class will load biblatex for the bibliography
% management. This is a powerful package, see its documentation for
% details. The styles will be setup correctly and automatically by
% choosing one of the two style keys as described earlier.
%
% In order for the bibliography to work, run latex in the following
% order (which is the standard order):
% 
% > lualatex thesis-example
% > bibtex thesis-example
% > lualatex thesis-example
% 
% Add BibTeX files using \addbibresource or use the {bibtex entries}
% environment (see below).
%
%%%
%
% Although everyting is normally setup automatically, you can change
% the options passed to biblatex using the key 'biblatex';
% for instance,
%
%   \UzLThesisSetup{biblatex={firstinits=false}}
%
% will switch off shortened first names. Normally, you will not need
% this key in your preamble. 
% 
% Note that the bibtex program is used as the 'backend' of biblatex
% by default (rather than biber, which is the preferred program of
% biblatex). This means that you can (and must) run *bibtex* after you
% have run lualatex on your thesis. If you wish to use biber instead
% of bibtex, say 'biblatex={backend=biber}'. 
% 
%%%
%
% The following environment is optional. It allows you to keep the
% bibtex entries for your thesis right here in the thesis file. What
% happens is that each time this tex file is processed, the contents
% of the following environment gets written to the file
% \jobname-bibtex-entries.bib (this file gets overwritten each
% time). Independently, \addbibresource{\jobname-bibtex-entries.bib}
% is always called if the file \jobname-bibtex-entries.bib
% exists. 
%
% In result, you can edit and keep the bibliography's bibtex entries
% right here. If you change something here, run latex, then bibtex,
% then latex once more.
%
% If you would like to manage the bibtex entries in a separate file,
% remove the below environment, delete the \jobname-bibtex-entries.bib
% file and instead write
%
% \addbibresource{filename-of-your-bibtex-file.bib}
%
% in the preamble.
%
%%%


\end{document}
